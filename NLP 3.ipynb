{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP Practice with NLTK library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data.txt\", \"r\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Project Gutenberg EBook of Man to Man, by Jackson Gregory\\n\\nThis eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.org\\n\\n\\nTitle: Man to Man\\n\\nAuthor: Jackson Gregory\\n\\nRelease Date: July 29, 2006 [EBook #18933]\\n\\nLanguage: English\\n\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK MAN TO MAN ***\\n\\n\\n\\n\\nProduced by Al Haines\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Frontispiece: The blazing heat was such that men and horses and steers\\nsuffered terribly.]\\n\\n\\n\\n\\n\\n\\nMAN TO MAN\\n\\n\\nBY\\n\\nJACKSON GREGORY\\n\\n\\n\\nAUTHOR OF\\n\\nJUDITH OF BLUE LAKE RANCH, THE BELLS OF SAN JUAN, SIX FEET FOUR, ETC.\\n\\n\\n\\n\\nILLUSTRATED BY\\n\\nJ. G. SHEPHERD\\n\\n\\n\\n\\n\\nGROSSET & DUNLAP\\n\\nPUBLISHERS -------- NEW YORK\\n\\n\\n\\n\\nCOPYRIGHT, 1920, BY\\n\\nCHARLES SCRIBNER'S SONS\\n\\n\\nPublished October, 1920\\n\\n\\n\\n\\nCONTENTS\\n\\n\\nCHAPTER\\n\\n     I. STEVE DIVES INTO DEEP WATERS\\n    II. MISS BLUE CLOAK KNOWS WHEN SHE'S BEAT\\n \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436574"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5551"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of man to man, by jackson gregory\\n\\nthis ebook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.',\n",
       " 'you may copy it, give it away or\\nre-use it under the terms of the project gutenberg license included\\nwith this ebook or online at www.gutenberg.org\\n\\n\\ntitle: man to man\\n\\nauthor: jackson gregory\\n\\nrelease date: july 29, 2006 [ebook #18933]\\n\\nlanguage: english\\n\\n\\n*** start of this project gutenberg ebook man to man ***\\n\\n\\n\\n\\nproduced by al haines\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[frontispiece: the blazing heat was such that men and horses and steers\\nsuffered terribly.]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in sentences:\n",
    "    sent = word_tokenize(i)\n",
    "    corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'of',\n",
       "  'man',\n",
       "  'to',\n",
       "  'man',\n",
       "  ',',\n",
       "  'by',\n",
       "  'jackson',\n",
       "  'gregory',\n",
       "  'this',\n",
       "  'ebook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with',\n",
       "  'almost',\n",
       "  'no',\n",
       "  'restrictions',\n",
       "  'whatsoever',\n",
       "  '.'],\n",
       " ['you',\n",
       "  'may',\n",
       "  'copy',\n",
       "  'it',\n",
       "  ',',\n",
       "  'give',\n",
       "  'it',\n",
       "  'away',\n",
       "  'or',\n",
       "  're-use',\n",
       "  'it',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'license',\n",
       "  'included',\n",
       "  'with',\n",
       "  'this',\n",
       "  'ebook',\n",
       "  'or',\n",
       "  'online',\n",
       "  'at',\n",
       "  'www.gutenberg.org',\n",
       "  'title',\n",
       "  ':',\n",
       "  'man',\n",
       "  'to',\n",
       "  'man',\n",
       "  'author',\n",
       "  ':',\n",
       "  'jackson',\n",
       "  'gregory',\n",
       "  'release',\n",
       "  'date',\n",
       "  ':',\n",
       "  'july',\n",
       "  '29',\n",
       "  ',',\n",
       "  '2006',\n",
       "  '[',\n",
       "  'ebook',\n",
       "  '#',\n",
       "  '18933',\n",
       "  ']',\n",
       "  'language',\n",
       "  ':',\n",
       "  'english',\n",
       "  '***',\n",
       "  'start',\n",
       "  'of',\n",
       "  'this',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'man',\n",
       "  'to',\n",
       "  'man',\n",
       "  '***',\n",
       "  'produced',\n",
       "  'by',\n",
       "  'al',\n",
       "  'haines',\n",
       "  '[',\n",
       "  'frontispiece',\n",
       "  ':',\n",
       "  'the',\n",
       "  'blazing',\n",
       "  'heat',\n",
       "  'was',\n",
       "  'such',\n",
       "  'that',\n",
       "  'men',\n",
       "  'and',\n",
       "  'horses',\n",
       "  'and',\n",
       "  'steers',\n",
       "  'suffered',\n",
       "  'terribly',\n",
       "  '.',\n",
       "  ']']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Tokenization of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97042"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus has 97042 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'man',\n",
       " 'to',\n",
       " 'man',\n",
       " ',',\n",
       " 'by',\n",
       " 'jackson',\n",
       " 'gregory',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_alt = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can implement tokenizatoin even with split() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'man',\n",
       " 'to',\n",
       " 'man,',\n",
       " 'by',\n",
       " 'jackson']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_alt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78078"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Punctuation and numbers and chars like \"*\", \"#\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_1 = [w for w in corpus if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'man',\n",
       " 'to',\n",
       " 'man',\n",
       " 'by',\n",
       " 'jackson',\n",
       " 'gregory',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_1[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_2 = [t for t in corpus_1 if t not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38117"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'man',\n",
       " 'man',\n",
       " 'jackson',\n",
       " 'gregory',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'terms']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_3 = [st.stem(t) for t in corpus_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'man',\n",
       " 'man',\n",
       " 'jackson',\n",
       " 'gregori',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyon',\n",
       " 'anywher',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrict',\n",
       " 'whatsoev',\n",
       " 'may',\n",
       " 'copi',\n",
       " 'give',\n",
       " 'away',\n",
       " 'term']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_3[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_4 = [lem.lemmatize(t) for t in corpus_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'man',\n",
       " 'man',\n",
       " 'jackson',\n",
       " 'gregory',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restriction',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'term']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_4[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38117"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Common Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('steve', 542), ('packard', 541), ('blenham', 524), ('man', 445), ('terry', 409)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(corpus_4).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**joining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'man',\n",
       " 'man',\n",
       " 'jackson',\n",
       " 'gregory',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restriction',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'term']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_4[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \" \".join(corpus_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project gutenberg ebook man man jackson gregory ebook use anyone anywhere cost almost restriction wh'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of Speech Tagging (PoST)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Steven Paul Jobs (/dʒɒbz/; February 24, 1955 – October 5, 2011) was an American business magnate, industrial designer, investor, and media proprietor. He was the chairman, chief executive officer (CEO), and co-founder of Apple Inc., the chairman and majority shareholder of Pixar, a member of The Walt Disney Company's board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT. Jobs is widely recognized as a pioneer of the personal computer revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Steven Paul Jobs (/dʒɒbz/; February 24, 1955 – October 5, 2011) was an American business magnate, industrial designer, investor, and media proprietor. He was the chairman, chief executive officer (CEO), and co-founder of Apple Inc., the chairman and majority shareholder of Pixar, a member of The Walt Disney Company's board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT. Jobs is widely recognized as a pioneer of the personal computer revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens=nltk.word_tokenize(text)\n",
    "tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steven Paul Jobs February October American business magnate industrial designer investor medium proprietor He chairman chief executive officer CEO Apple chairman majority shareholder Pixar member The Walt Disney Company board director following acquisition Pixar founder chairman CEO NeXT Jobs widely recognized pioneer personal computer revolution along Apple Steve Wozniak'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Steven', 'NNP'),\n",
       " ('Paul', 'NNP'),\n",
       " ('Jobs', 'NNP'),\n",
       " ('February', 'NNP'),\n",
       " ('October', 'NNP'),\n",
       " ('American', 'NNP'),\n",
       " ('business', 'NN'),\n",
       " ('magnate', 'NN'),\n",
       " ('industrial', 'JJ'),\n",
       " ('designer', 'NN'),\n",
       " ('investor', 'NN'),\n",
       " ('medium', 'NN'),\n",
       " ('proprietor', 'NN'),\n",
       " ('He', 'PRP'),\n",
       " ('chairman', 'NN'),\n",
       " ('chief', 'JJ'),\n",
       " ('executive', 'JJ'),\n",
       " ('officer', 'NN'),\n",
       " ('CEO', 'NNP'),\n",
       " ('Apple', 'NNP'),\n",
       " ('chairman', 'NN'),\n",
       " ('majority', 'NN'),\n",
       " ('shareholder', 'NN'),\n",
       " ('Pixar', 'NNP'),\n",
       " ('member', 'NN'),\n",
       " ('The', 'DT'),\n",
       " ('Walt', 'NNP'),\n",
       " ('Disney', 'NNP'),\n",
       " ('Company', 'NNP'),\n",
       " ('board', 'NN'),\n",
       " ('director', 'NN'),\n",
       " ('following', 'VBG'),\n",
       " ('acquisition', 'NN'),\n",
       " ('Pixar', 'NNP'),\n",
       " ('founder', 'NN'),\n",
       " ('chairman', 'NN'),\n",
       " ('CEO', 'NNP'),\n",
       " ('NeXT', 'NNP'),\n",
       " ('Jobs', 'NNP'),\n",
       " ('widely', 'RB'),\n",
       " ('recognized', 'VBD'),\n",
       " ('pioneer', 'NN'),\n",
       " ('personal', 'JJ'),\n",
       " ('computer', 'NN'),\n",
       " ('revolution', 'NN'),\n",
       " ('along', 'IN'),\n",
       " ('Apple', 'NNP'),\n",
       " ('Steve', 'NNP'),\n",
       " ('Wozniak', 'NNP')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCC     coordinating conjunction\\nCD     cardinal digit\\nDT     determiner\\nEX     existential there (like: \"there is\" ... think of it like \"there exists\")\\nFW     foreign word\\nIN     preposition/subordinating conjunction\\nJJ     adjective \\'big\\'\\nJJR    adjective, comparative \\'bigger\\'\\nJJS    adjective, superlative \\'biggest\\'\\nLS     list marker 1)\\nMD     modal could, will\\nNN     noun, singular \\'desk\\'\\nNNS    noun plural \\'desks\\'\\nNNP    proper noun, singular \\'Harrison\\'\\nNNPS   proper noun, plural \\'Americans\\'\\nPDT    predeterminer \\'all the kids\\'\\nPOS    possessive ending parent\\'s\\nPRP    personal pronoun I, he, she\\nPRP$   possessive pronoun my, his, hers\\nRB     adverb very, silently,\\nRBR    adverb, comparative better\\nRBS    adverb, superlative best\\nRP     particle give up\\nTO     to go \\'to\\' the store.\\nUH     interjection errrrrrrrm\\nVB     verb, base form take\\nVBD    verb, past tense took\\nVBG    verb, gerund/present participle taking\\nVBN    verb, past participle taken\\nVBP    verb, sing. present, non-3d take\\nVBZ    verb, 3rd person sing. present takes\\nWDT    wh-determiner which\\nWP     wh-pronoun who, what\\nWP$    possessive wh-pronoun whose\\nWRB    wh-abverb where, when\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CC     coordinating conjunction\n",
    "CD     cardinal digit\n",
    "DT     determiner\n",
    "EX     existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW     foreign word\n",
    "IN     preposition/subordinating conjunction\n",
    "JJ     adjective 'big'\n",
    "JJR    adjective, comparative 'bigger'\n",
    "JJS    adjective, superlative 'biggest'\n",
    "LS     list marker 1)\n",
    "MD     modal could, will\n",
    "NN     noun, singular 'desk'\n",
    "NNS    noun plural 'desks'\n",
    "NNP    proper noun, singular 'Harrison'\n",
    "NNPS   proper noun, plural 'Americans'\n",
    "PDT    predeterminer 'all the kids'\n",
    "POS    possessive ending parent's\n",
    "PRP    personal pronoun I, he, she\n",
    "PRP$   possessive pronoun my, his, hers\n",
    "RB     adverb very, silently,\n",
    "RBR    adverb, comparative better\n",
    "RBS    adverb, superlative best\n",
    "RP     particle give up\n",
    "TO     to go 'to' the store.\n",
    "UH     interjection errrrrrrrm\n",
    "VB     verb, base form take\n",
    "VBD    verb, past tense took\n",
    "VBG    verb, gerund/present participle taking\n",
    "VBN    verb, past participle taken\n",
    "VBP    verb, sing. present, non-3d take\n",
    "VBZ    verb, 3rd person sing. present takes\n",
    "WDT    wh-determiner which\n",
    "WP     wh-pronoun who, what\n",
    "WP$    possessive wh-pronoun whose\n",
    "WRB    wh-abverb where, when\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NER (Named Entity Regocnition)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Error with downloaded zip file\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pc/nltk_data'\n    - 'C:\\\\Program Files\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-a7415786881c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnamed_ent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\anaconda\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pc/nltk_data'\n    - 'C:\\\\Program Files\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "named_ent = nltk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(named_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \" \".join(corpus_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(corpus_4).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000)\n",
    "wordcloud.generate(original)\n",
    "\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
